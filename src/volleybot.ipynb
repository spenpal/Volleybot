{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMnCjl9Nqx_g"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks2DTAF-ret4"
      },
      "source": [
        "**IMPORT STATEMENTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD-B3Y91rJlQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# IMPORTS\n",
        "from collections import Counter\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from string import punctuation\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "# Settings and Data Loading\n",
        "warnings.filterwarnings('ignore')\n",
        "data = pickle.load(open('../data/vball_kb.pickle', 'rb'))\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load Knowledge Base Into List\n",
        "data_list = []\n",
        "for sents in data.values():\n",
        "  data_list.extend(sents)\n",
        "data_list = list(set(data_list))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SpUBjKtfrspD"
      },
      "source": [
        "**FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2HUvOoUcUGH"
      },
      "outputs": [],
      "source": [
        "# Vader Sentiment Function \n",
        "def preprocessing_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  important_tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
        "  wnl = WordNetLemmatizer()\n",
        "  lemmatized = [wnl.lemmatize(t) for t in important_tokens]\n",
        "  str_list = ' '.join([str(elem) for elem in lemmatized])\n",
        "  return str_list\n",
        "\n",
        "def sentence_sentiment(text):\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  vs = analyzer.polarity_scores(text)\n",
        "  max_key = ''\n",
        "  if vs.get('neg') > 0 or vs.get('pos') > 0:\n",
        "    if vs.get('neg') > vs.get('pos'):\n",
        "      max_key = 'neg'\n",
        "    else:\n",
        "      max_key = 'pos'\n",
        "  else:\n",
        "    max_key = max(vs, key=vs.get)\n",
        "  return max_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX4xKmEqRXgp"
      },
      "outputs": [],
      "source": [
        "# POS Function\n",
        "def get_hotwords(text):\n",
        "    result = []\n",
        "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB'] # 1\n",
        "    doc = nlp(text.lower()) # 2\n",
        "    for token in doc:\n",
        "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation): # Text Preprocessing\n",
        "            continue\n",
        "        if(token.pos_ in pos_tag):\n",
        "            result.append(token.text)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6VOhk1KrsPH"
      },
      "outputs": [],
      "source": [
        "# Greetings\n",
        "def greeting_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  bot_greetings = ['howdy', 'hi', 'hey', 'hello', 'hola']\n",
        "  user_greetings = ['hi', 'hey', 'hello', 'hola', 'greetings', 'wassup', 'whats good', 'sup', 'yo']\n",
        "\n",
        "  if any(word in user_greetings for word in text.split()):\n",
        "    return random.choice(bot_greetings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHuf6W5ztLHe"
      },
      "outputs": [],
      "source": [
        "# Bot Responses\n",
        "def index_sort(lst):\n",
        "  lst_and_idx = [(num, idx) for idx, num in enumerate(lst)]\n",
        "  lst_and_idx.sort(reverse=True)\n",
        "  lst_idx = [tup[1] for tup in lst_and_idx]\n",
        "  return lst_idx\n",
        "\n",
        "def bot_response(user_input):\n",
        "  # Put user_input in the data\n",
        "  user_input = user_input.lower()\n",
        "  data_list.append(user_input)\n",
        "\n",
        "  # Find similaritiy score between user_input and all sentence data, using cosine similarity\n",
        "  count_matrix = CountVectorizer().fit_transform(data_list)\n",
        "  similarity_scores = cosine_similarity(count_matrix[-1], count_matrix)\n",
        "  similarity_scores_list = similarity_scores.flatten()\n",
        "\n",
        "  # Get the value of indexes, from highest to lowest similarity score\n",
        "  index = index_sort(similarity_scores_list)\n",
        "  index = index[1:]\n",
        "\n",
        "  # Grab highest similar sentences to user's query. Limit it to only 2 sentences\n",
        "  sentence_limit, bot_response = 0, ''\n",
        "  for i in range(len(index)):\n",
        "    if similarity_scores_list[index[i]] > 0.0:\n",
        "      bot_response = f'{bot_response} {data_list[index[i]]}'\n",
        "      sentence_limit += 1\n",
        "    if sentence_limit > 1:\n",
        "      break\n",
        "\n",
        "  # If there are no similar sentences, return nothing to be found\n",
        "  if not bot_response:\n",
        "    bot_response = 'I am sorry, but I do not understand.'\n",
        "\n",
        "  # Remove user input from data KB\n",
        "  data_list.remove(user_input)\n",
        "\n",
        "  return bot_response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxcDYooWa0p"
      },
      "source": [
        "**USER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfbhdrd0Wh8T"
      },
      "outputs": [],
      "source": [
        "class User:\n",
        "  def __init__(self, name, keywords, likes_list, dislike_list):\n",
        "    self.name = name.title()\n",
        "    self.keywords = keywords\n",
        "    self.likes_list = likes_list\n",
        "    self.dislike_list = dislike_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbavet-j-wLY"
      },
      "outputs": [],
      "source": [
        "users = []\n",
        "liked_items = []\n",
        "disliked_items = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DGndwXkUzqbV"
      },
      "source": [
        "**MAIN**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ICkUOPWLLagS"
      },
      "source": [
        "To access your user model, have one conversation with the bot, then exit out.\n",
        "\n",
        "Then, re-run this cell, and input your name again, and it will show your user model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptFfKa_izr8d"
      },
      "outputs": [],
      "source": [
        "# Chatbot Code\n",
        "print('Type \"EXIT\" or \"exit\" once you are done talking to the bot')\n",
        "print(\"-> Volleybot: Hey, my name is VolleyBot. Nice to meet you. What is your name?\")\n",
        "name_input = input('-> Me: ').split()[-1]\n",
        "\n",
        "if any(user.name == name_input for user in users):\n",
        "  for user in users:\n",
        "    if user.name == name_input:\n",
        "      print(f'-> Volleybot: Welcome back, {user.name}! Here is what I got from our last session!')\n",
        "      print(f'Your Likes: {user1.likes_list}')\n",
        "      print(f'Your Dislikes: {user1.dislike_list}')\n",
        "else:\n",
        "  print(f\"-> Volleybot: Hey {name_input}, got any questions about volleyball?\")\n",
        "\n",
        "while True:\n",
        "  try:\n",
        "    print('-> Me: ', end='')\n",
        "    user_input = input()\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "      raise SystemExit\n",
        "\n",
        "    if sentence_sentiment(preprocessing_text(user_input)) == 'neg':\n",
        "      disliked_items.append(get_hotwords(user_input))\n",
        "    elif sentence_sentiment(preprocessing_text(user_input)) == 'pos':\n",
        "      liked_items.append(get_hotwords(user_input))\n",
        "\n",
        "    if greeting_response(user_input):\n",
        "      print(f'-> Volleybot: {greeting_response(user_input)}')\n",
        "    else:\n",
        "      print(f'-> Volleybot: {bot_response(user_input)}')\n",
        "\n",
        "  except(KeyboardInterrupt, EOFError, SystemExit):\n",
        "    user1 = User(name_input, liked_items + disliked_items, liked_items, disliked_items)\n",
        "    users.append(user1)\n",
        "    print(f'-> Volleybot: Adios! It was good chatting with you, {user1.name}!')\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "VolleyBot",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
